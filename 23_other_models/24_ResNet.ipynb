{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.7123 - loss: 0.5525 - val_accuracy: 0.7138 - val_loss: 0.5591\n",
      "Epoch 2/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.8020 - loss: 0.4316 - val_accuracy: 0.7964 - val_loss: 0.4327\n",
      "Epoch 3/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.8277 - loss: 0.3852 - val_accuracy: 0.8280 - val_loss: 0.3690\n",
      "Epoch 4/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.8401 - loss: 0.3615 - val_accuracy: 0.8322 - val_loss: 0.3662\n",
      "Epoch 5/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.8581 - loss: 0.3258 - val_accuracy: 0.8580 - val_loss: 0.3191\n",
      "Epoch 6/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.8748 - loss: 0.3149 - val_accuracy: 0.8459 - val_loss: 0.3395\n",
      "Epoch 7/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.8856 - loss: 0.2736 - val_accuracy: 0.8732 - val_loss: 0.3050\n",
      "Epoch 8/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.8867 - loss: 0.2707 - val_accuracy: 0.8743 - val_loss: 0.3091\n",
      "Epoch 9/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9023 - loss: 0.2461 - val_accuracy: 0.8722 - val_loss: 0.3060\n",
      "Epoch 10/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9002 - loss: 0.2441 - val_accuracy: 0.8627 - val_loss: 0.3237\n",
      "Epoch 11/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9100 - loss: 0.2239 - val_accuracy: 0.8780 - val_loss: 0.3072\n",
      "Epoch 12/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9091 - loss: 0.2257 - val_accuracy: 0.8822 - val_loss: 0.2862\n",
      "Epoch 13/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9219 - loss: 0.1952 - val_accuracy: 0.8916 - val_loss: 0.2718\n",
      "Epoch 14/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9194 - loss: 0.1919 - val_accuracy: 0.8964 - val_loss: 0.2557\n",
      "Epoch 15/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9252 - loss: 0.1907 - val_accuracy: 0.9053 - val_loss: 0.2583\n",
      "Epoch 16/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9343 - loss: 0.1734 - val_accuracy: 0.8722 - val_loss: 0.3209\n",
      "Epoch 17/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9351 - loss: 0.1653 - val_accuracy: 0.8990 - val_loss: 0.2733\n",
      "Epoch 18/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9362 - loss: 0.1612 - val_accuracy: 0.9053 - val_loss: 0.2494\n",
      "Epoch 19/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9347 - loss: 0.1610 - val_accuracy: 0.9022 - val_loss: 0.2510\n",
      "Epoch 20/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9394 - loss: 0.1554 - val_accuracy: 0.9069 - val_loss: 0.2563\n",
      "Epoch 21/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9371 - loss: 0.1570 - val_accuracy: 0.8901 - val_loss: 0.2976\n",
      "Epoch 22/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9418 - loss: 0.1503 - val_accuracy: 0.9037 - val_loss: 0.2455\n",
      "Epoch 23/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9504 - loss: 0.1275 - val_accuracy: 0.9164 - val_loss: 0.2294\n",
      "Epoch 24/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9502 - loss: 0.1405 - val_accuracy: 0.9037 - val_loss: 0.2498\n",
      "Epoch 25/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9497 - loss: 0.1271 - val_accuracy: 0.9111 - val_loss: 0.2600\n",
      "Epoch 26/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9451 - loss: 0.1428 - val_accuracy: 0.9237 - val_loss: 0.2319\n",
      "Epoch 27/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9496 - loss: 0.1297 - val_accuracy: 0.9164 - val_loss: 0.2447\n",
      "Epoch 28/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9516 - loss: 0.1195 - val_accuracy: 0.9048 - val_loss: 0.2645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "[[885  92]\n",
      " [ 67 857]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       977\n",
      "           1       0.90      0.93      0.92       924\n",
      "\n",
      "    accuracy                           0.92      1901\n",
      "   macro avg       0.92      0.92      0.92      1901\n",
      "weighted avg       0.92      0.92      0.92      1901\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, BatchNormalization, ReLU, Add\n",
    "from tensorflow.keras.models import Model\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "def resnet_block(inputs, filters=64, kernel_size=3, strides=1):\n",
    "    x = Conv1D(filters, kernel_size, padding='same', strides=strides)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = Conv1D(filters, kernel_size, padding='same', strides=strides)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Add skip connection\n",
    "    x = Add()([x, inputs])\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('Cleaned_Train_DB_80.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Load data into df\n",
    "cursor.execute(\"SELECT * FROM mockdata\")\n",
    "rows = cursor.fetchall()\n",
    "columns = ['n', 'timestamp', 'ax', 'ay', 'az', 'gx', 'gy', 'gz', 'label']\n",
    "df = pd.DataFrame(rows, columns=columns)\n",
    "conn.close()\n",
    "\n",
    "# Preprocessing\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "# Split data into features (X) and labels (y)\n",
    "X = df[['ax', 'ay', 'az', 'gx', 'gy', 'gz']]\n",
    "y = df['label']\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Save the scaler to a file\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "# Split the data \n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape data for Conv1D \n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "\n",
    "# Define the ResNet model\n",
    "inputs = Input(shape=(X_train.shape[1], 1))\n",
    "x = Conv1D(64, 3, padding='same')(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = ReLU()(x)\n",
    "\n",
    "# Add multiple ResNet blocks\n",
    "for _ in range(3):\n",
    "    x = resnet_block(x)\n",
    "\n",
    "x = Conv1D(64, 3, padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = ReLU()(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "outputs = Dense(len(label_encoder.classes_), activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, \n",
    "          validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "# Save the model\n",
    "model.save('ResNet_model.h5')\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_pred = model.predict(X_val)\n",
    "y_pred_classes = y_pred.argmax(axis=-1)\n",
    "\n",
    "print(confusion_matrix(y_val, y_pred_classes))\n",
    "print(classification_report(y_val, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model on new Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step\n",
      "['normal']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import pandas as pd\n",
    "\n",
    "new_data = {\n",
    "    'ax': [-0.304688],\n",
    "    'ay': [0.424316],\n",
    "    'az': [-0.908691],\n",
    "    'gx': [-6.793893],\n",
    "    'gy': [-0.725191],\n",
    "    'gz': [0.618321]\n",
    "}\n",
    "new_df = pd.DataFrame(new_data)\n",
    "model = load_model('ResNet_model.h5')\n",
    "predictions = model.predict(new_df)\n",
    "predicted_labels = label_encoder.inverse_transform(predictions.argmax(axis=1))\n",
    "print(predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "Timestamp: 20-05-2024 13:19:45:65564 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:20:54:18225 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:16:47:42277 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:15:19:91737 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:16:36:77520 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:23:05:88818 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:23:38:61220 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:17:39:73074 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:15:06:10220 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:19:32:35887 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:21:36:17714 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:16:18:67092 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:21:51:52998 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:15:41:40409 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:17:38:60793 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:18:03:19136 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:16:43:41287 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:17:53:54333 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:17:45:79177 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:19:15:41242 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:23:29:38151 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:17:05:78391 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:23:53:92561 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:17:10:52710 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:19:48:14871 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:18:51:44105 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:19:13:91447 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:15:28:05606 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:17:04:30434 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:17:31:46545 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:16:42:35544 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:21:51:06799 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:21:11:16895 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:22:56:61428 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:24:34:91144 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:22:53:27608 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:23:27:84782 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:22:53:56646 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:21:19:83158 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:24:32:91234 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:17:00:21935 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:18:29:32094 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:18:13:90474 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:16:25:97191 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:19:20:06387 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:19:03:09136 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:17:31:02751 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:19:30:82040 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:21:18:24647 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:16:33:12689 => Prediction: anomaly\n",
      "Timestamp: 20-05-2024 13:23:06:33177 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:24:42:09344 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:23:03:75166 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:21:36:16706 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:17:54:57206 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:23:43:24943 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:22:21:81234 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:22:10:46174 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:18:32:90296 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:23:48:36514 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:24:37:41199 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:24:53:27427 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:16:07:49583 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:21:33:63828 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:19:46:16557 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:16:18:22273 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:15:57:89186 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:17:17:19510 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:23:26:25792 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:22:10:50576 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:23:05:33323 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:17:25:81480 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:15:35:77959 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:17:13:62774 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:16:38:71507 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:19:05:66145 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:23:34:47572 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:18:21:20837 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:20:42:00352 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:21:28:07559 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:23:44:67921 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:22:52:03132 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:16:49:47952 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:24:47:17783 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:18:34:40742 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:15:32:25199 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:23:57:99592 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:22:45:81573 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:17:32:46052 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:20:51:12409 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:20:52:20270 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:22:33:13717 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:22:04:83770 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:24:17:96455 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:20:49:61850 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:19:17:98170 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:18:25:07273 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:23:40:64570 => Prediction: normal\n",
      "Timestamp: 20-05-2024 13:20:39:41190 => Prediction: anomaly\n",
      "Timestamp: 20-05-2024 13:22:17:71878 => Prediction: normal\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Database file \n",
    "database_file = 'testData.db'\n",
    "model_file = 'ResNet_model.h5'\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(database_file)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Fetch data from the table \n",
    "query = 'SELECT n, timestamp, ax, ay, az, gx, gy, gz FROM test_data LIMIT 100'\n",
    "cursor.execute(query)\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Convert rows to DataFrame\n",
    "columns = ['n', 'timestamp', 'ax', 'ay', 'az', 'gx', 'gy', 'gz']\n",
    "df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "# Preprocess the data\n",
    "X = df[['ax', 'ay', 'az', 'gx', 'gy', 'gz']].values\n",
    "\n",
    "# Load the scaler used during training\n",
    "scaler = StandardScaler()\n",
    "# Random Training Data\n",
    "sample_training_data = np.random.rand(500, 6)\n",
    "scaler.fit(sample_training_data)\n",
    "\n",
    "# Normalize the features\n",
    "X = scaler.transform(X)\n",
    "\n",
    "# Reshape data for Conv1D (samples, timesteps, channels)\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model(model_file)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X)\n",
    "y_pred_classes = y_pred.argmax(axis=-1)\n",
    "\n",
    "# Output results\n",
    "labels = ['normal', 'anomaly']\n",
    "\n",
    "for i, pred in enumerate(y_pred_classes):\n",
    "    print(f\"Timestamp: {df.iloc[i]['timestamp']} => Prediction: {labels[pred]}\")\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
